name: Deploy to DEV

concurrency: 1

on:
  workflow_dispatch:

  pull_request:
    types:
      - opened
      - synchronize
    branches:
      - main
    paths:
      - "dabs_sample_project/resources/*.yml"
      - "dabs_sample_project/src/*"
      - "dabs-tiny-proj/hello.py"

jobs:
  deploy:
    name: "Deploy bundle"
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - uses: databricks/setup-cli@main
      
      - run: databricks bundle deploy -t dev
        working-directory: dabs_sample_project/
        env:
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_BUNDLE_ENV: dev

      - run: databricks bundle deploy -t dev
        working-directory: dabs-tiny-proj/
        env:
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_BUNDLE_ENV: dev
      
#   test_jobs:
#     name: "Test jobs"
#     runs-on: ubuntu-latest

#     needs:
#       - deploy

#     steps:
#       - uses: actions/checkout@v3

#       - uses: actions/setup-python@v4
#         with:
#           python-version: 3.11
#           cache: 'pip'

#       - run: pip install -r requirements.txt
#         working-directory: .github/support

#       - uses: databricks/setup-cli@main

#       - shell: bash
#         name: Test Jobs
#         run: |
#           set -o pipefail
#           databricks bundle run -t dev datakickstart_dabs_job --refresh-all 2>&1 | tee output.log
#           databricks bundle run -t dev notebook_validation_job --refresh-all 2>&1 | tee output.log
#         env:
#           DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
#           DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
#           DATABRICKS_BUNDLE_ENV: dev
